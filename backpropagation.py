# -*- coding: utf-8 -*-
"""backpropagation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Za5DHdnVdV3H6U-7vziVO6E_O9m_-XGd
"""

class Mul():

  def __init__(self):
    self.x = None
    self.y = None
  
  def forward(self, x, y):
    self.x = x
    self.y = y
    result = x * y
    return result

  def backward(self, dresult):
    dx = dresult * self.y
    dy = dresult * self.x
    return dx,dy

class Add():

  def __init__(self):
    self.x = None
    self.y = None
  
  def forward(self, x, y):
    self.x = x
    self.y = y
    result = x + y
    return result

  def backward(self,dresult):
    dx = dresult
    dy = dresult
    return dx, dy

a,b,c = -1, 3, 4

x = Add()
y = Add()
f = Mul()

x_result = x.forward(a,b)
y_result = y.forward(b,c)

print(x_result)
print(y_result)
print(f.forward(x_result,y_result))

dresult = 1

dx_mul,dy_mul = f.backward(dresult)

da_add, db_add_1 = x.backward(dx_mul)

db_add_2,dc_add = y.backward(dy_mul)

print(dx_mul, dy_mul)
print(da_add)
print(db_add_1+db_add_2)
print(dc_add)

import numpy as np
class Sigmoid():

  def __init__(self):
    self.out = None

  def forward(self,x):
    out = 1/ (1+np.exp(-x))
    return out
  
  def backward(self,dout):
    dx = dout * (1,0 - self.out) * self.dout
    return dx

class ReLu():

  def __init__(self):
    self.out = None

  def forward(self,x):
    self.mask = (x<0)
    out = x.copy()
    out[x<0] = 0
    return out
  
  def backward(self,dout):
    dout[self.mask] = 0
    dx = dout
    return dx

x = np.random.rand(3)
w = np.random.rand(3,2)

b = np.random.rand(2)

print(x.shape)
print(w.shape)
print(b.shape)

y = np.dot(x,w) + b
print(y.shape)

x = np.random.rand(3)
w = np.random.rand(2,2)

b = np.random.rand(2)

print(x.shape)
print(w.shape)
print(b.shape)

y = np.dot(x,w) + b
print(y.shape)

x = np.random.randn(2)
w = np.random.randn(2,3)
y = np.dot(x,w)

print(x)
print(w)
print(y)

dL_dY = np.random.randn(3)
dL_dX = np.dot(dL_dY,w.T)
dL_dW = np.dot(x.reshape(-1,1),dL_dY.reshape(1,-1))

print("dL_dY\n{}".format(dL_dY))
print("dL_dX\n{}".format(dL_dX))
print("dL_dW\n{}".format(dL_dW))

x = np.random.randn(2)
w = np.random.randn(2,3)
b = np.random.randn(3)
y = np.dot(x,w) +b
print(y)

dL_dY = np.random.randn(3)
dL_dX = np.dot(dL_dY,w.T)
dL_dW = np.dot(x.reshape(-1,1),dL_dY.reshape(1,-1))
dL_dB = dL_dY

print("dL_dY\n{}".format(dL_dY))
print("dL_dx\n{}".format(dL_dX))
print("dL_dw\n{}".format(dL_dW))
print("dL_db\n{}".format(dL_dB))

x = np.random.randn(4,3)
w = np.random.randn(3,2)
b = np.random.randn(2)

y = np.dot(x,w) +b
print(y)

dL_dY = np.random.randn(4,2)
dL_dX = np.dot(dL_dY,w.T)
dL_dW = np.dot(x.T,dL_dY)
dL_dB = np.sum(dL_dY,axis=0)

print("dL_dY\n{}".format(dL_dY))
print("dL_dx\n{}".format(dL_dX))
print("dL_dw\n{}".format(dL_dW))
print("dL_db\n{}".format(dL_dB))

class Layer():

  def __init__(self):
    self.w = np.random.randn(3,2)
    self.b = np.random.randn(2)
    self.x = None
    self.dw = None
    self.db = None

  def forward(self,x):
    self.x = x
    out = np.dot(x,self.w) + b
    return out
  
  def backward(self,dout):
    dx = np.dot(dout,self.w.T)
    self.db = np.sum(dout,axis = 0)
    return dx

np.random.seed(0)

layer = Layer()

x = np.random.rand(2,3)
y = layer.forward(x)
print(x)

dout = np.random.rand(2,2)
dout_dx = layer.backward(dout)
print(dout_dx)

###################################################

#MNIST Multiclassification with backpropagation

####################################################

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

plt.style.use('seaborn-whitegrid')

from collections import OrderedDict

np.random.seed(1)

###### Importing the dataset ###############
mnist = tf.keras.datasets.mnist

(x_train,y_train), (x_test,y_test) = mnist.load_data()

num_classes = 10

######### Preprocessing #############

x_train, x_test = x_train.reshape(-1,28 * 28).astype(np.float32), x_test.reshape(-1,28 * 28).astype(np.float32)
x_train /= .255
x_test /= .255

y_train = np.eye(num_classes)[y_train]

print(x_train.shape)
print(y_train.shape)
print(x_test.shape)
print(y_test.shape)

############ Hyperparameter ################

epochs = 1000

lr = 1e-3

batch_size = 100

train_size = x_train.shape[0]

######### Utility Function #############

def softmax(x):
  if x.ndim == 2:
    x = x.T
    x = x - np.max(x, axis =0)
    y = np.exp(x) / np.sum(np.exp(x),axis=0)
    return y.T

  x = x- np.max(x)
  return np.exp(x) / np.sum(np.exp(x))


def MSE(pred_y,y):
  return 0.5 * np.sum((pred_y - y)**2)

def CEE(pred_y,y):
  if pred_y.ndim ==1:
    y = y.reshape(1,y.size)
    pred_y = pred_y.reshape(1,pred_y.size)

  if y.size == pred_y.size:
    y = y.argmax(axis=1)

  batch_size = pred_y.shape[0]

  return -np.sum(np.log(pred_y[np.arange(batch_size), y] + 1e-7)) / batch_size

def softmax_loss(x, y):
  pred_y = softmax(x)
  return CEE(pred_y,y)

class ReLu():

  def __init__(self):
    self.out = None

  def forward(self,x):
    self.mask = (x<0)
    out = x.copy()
    out[x<0] = 0
    return out
  
  def backward(self,dout):
    dout[self.mask] = 0
    dx = dout
    return dx

class Sigmoid():

  def __init__(self):
    self.out = None

  def forward(self,x):
    out = 1/ (1 + np.exp(-x))
    return out

  def backward(self,dout):
    dx = dout * (1.0 - self.out) * self.dout
    return dx

class Layer():

  def __init__(self,w,b):
    self.w = w
    self.b = b

    self.x = None
    self.origin_x_shape = None

    self.dl_dw = None
    self.dl_db = None

  def forward(self,x):
    self.origin_x_shape = x.shape

    x= x.reshape(x.shape[0],-1)

    self.x = x
    #print(self.x.shape, self.w.shape, self.b.shape)
    out = np.dot(self.x, self.w) + self.b

    return out

  def backward(self,dout):
    dx = np.dot(dout,self.w.T)
    self.dl_dw = np.dot(self.x.T,dout)
    self.dl_db = np.sum(dout,axis = 0)
    dx = dx.reshape(*self.origin_x_shape)
    return dx

class Softmax():

  def __init__(self):
    self.loss = None
    self.y = None
    self.t = None

  def forward(self,x,t):
    self.t = t
    self.y = softmax(x)
    self.loss = CEE(self.y,self.t)
    return self.loss

  def backward(self, dout = 1):
    batch_size = self.t.shape[0]

    if self.t.size == self.y.size:
      dx = (self.y  - self.t) / batch_size
    else:
      dx = self.y.copy()
      dx[np.arange(batch_size), self.t] -= 1
      dx = dx / batch_size

    return dx

class Network():

  def __init__(self, input_size, hidden_size, output_size, activation ='relu'):
    self.input_size = input_size
    self.output_size = output_size
    self.hidden_size = hidden_size
    self.hidden_layer_num = len(hidden_size)
    self.params={}

    self.__init_weights(activation)

    activation_layer = {'sigmoid': Sigmoid, 'relu': ReLu}
    self.layers = OrderedDict()

    for idx in range(1, self.hidden_layer_num +1):
      self.layers['Layer' + str(idx)] = Layer(self.params['W'+str(idx)], self.params['b'+str(idx)])
      self.layers['Activation_function' + str(idx)] = activation_layer[activation]()

    idx = self.hidden_layer_num +1

    self.layers['Layer'+str(idx)] = Layer(self.params['W'+str(idx)], self.params['b'+str(idx)])

    self.last_layer = Softmax()

  def __init_weights(self,activation):
    weight_std = None
    all_size_list = [self.input_size] + self.hidden_size + [self.output_size]
    for idx in range(1,len(all_size_list)):
      if activation.lower() == 'relu':
        weight_std = np.sqrt(2.0/ self.input_size)

      elif activation.lower() == 'sigmoid':
        weight_std = np.sqrt( 1.0 /self.input_size)

      self.params['W'+str(idx)] = weight_std * np.random.randn(all_size_list[idx-1],all_size_list[idx])
      self.params['b'+str(idx)] = np.random.randn(all_size_list[idx])

  def predict(self, x):
    for layer in self.layers.values():
      x = layer.forward(x)

    return x

  def loss(self,x,y):
    pred_y = self.predict(x)

    return self.last_layer.forward(pred_y,y)

  def accuracy(self,x,y):
    pred_y = self.predict(x)
    pred_y = np.argmax(pred_y,axis =1)

    if y.ndim !=1:
      y = np.argmax(y,axis=1)

    acc = np.sum(pred_y == y) / float(x.shape[0])

    return acc

  def gradient(self,x,t):
    self.loss(x,t)

    dout = 1
    dout = self.last_layer.backward(dout)
    #print(dout)
    layers = list(self.layers.values())
    layers.reverse()
    
    for layer in layers:
      dout = layer.backward(dout)

    grads = {}

    for idx in range(1, self.hidden_layer_num+2):
      grads['W'+str(idx)] = self.layers['Layer'+str(idx)].dl_dw
      grads['b'+str(idx)] = self.layers['Layer' + str(idx)].dl_db

    return grads

model = Network(28*28,[100,64,32],10, activation = 'relu')

train_loss_list = []
train_acc_list = []
test_acc_list = []

for epoch in range(epochs):
  batch_mask = np.random.choice(train_size, batch_size)
  x_batch = x_train[batch_mask]
  y_batch = y_train[batch_mask]
  #print(x_batch.shape, y_batch.shape)

  grad = model.gradient(x_batch, y_batch)
  #print(grad['W1'])
  for key in model.params.keys():
    model.params[key] -= lr * grad[key]

  loss = model.loss(x_batch, y_batch)
  train_loss_list.append(loss)

  if epoch % 50 == 0:
    train_acc = model.accuracy(x_train, y_train)
    test_acc = model.accuracy(x_test, y_test)
    train_acc_list.append(train_acc)
    test_acc_list.append(test_acc)
    print("Epoch: {}, Train Acc: {}, Test Acc:{:.4f}".format(epoch+1, train_acc, test_acc))

plt.plot(np.arange(1000/50),train_acc_list, 'r--', label = 'train_acc') 
plt.plot(np.arange(1000/50),test_acc_list, 'b', label = 'test_acc' )

plt.title('Result')
plt.xlabel('Epochs')
plt.legend(loc=5)

plt.grid()
plt.show()

plt.plot(np.arange(1000), train_loss_list, 'green', label='train_loss')
plt.title('train_loss')
plt.xlabel('Epochs')
plt.grid()
plt.show()

